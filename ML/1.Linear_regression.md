# Linear Regression in Machine Learning

## Concept

Linear regression is a fundamental supervised learning algorithm used for predicting a continuous target variable based on one or more input features. It assumes a linear relationship between the input variables (independent variables) and the output variable (dependent variable).

The model represents this relationship with a linear equation:
```
y = b0 + b1*x1 + b2*x2 + ... + bn*xn
```
Where:
- y is the dependent variable (target)
- x1, x2, ..., xn are the independent variables (features)
- b0 is the y-intercept
- b1, b2, ..., bn are the coefficients (weights)

### Types of Linear Regression:
1. **Simple Linear Regression**: Only one independent variable
2. **Multiple Linear Regression**: More than one independent variable

## Use Cases

Linear regression is widely used in:
- Predicting house prices based on features like size, location, etc.
- Sales forecasting based on advertising budget
- Risk assessment in insurance
- Stock price prediction
- Temperature prediction based on various weather parameters
- Understanding relationships between variables (e.g., education level vs. income)

## Models and Algorithms

### 1. Ordinary Least Squares (OLS)
- Minimizes the sum of squared residuals
- Direct closed-form solution: β = (XᵀX)⁻¹Xᵀy

### 2. Gradient Descent
- Iterative optimization approach
- Useful for large datasets where OLS is computationally expensive

### 3. Regularized Linear Regression
- **Ridge Regression (L2 regularization)**: Adds squared magnitude of coefficients to loss function
- **Lasso Regression (L1 regularization)**: Adds absolute magnitude of coefficients to loss function
- **Elastic Net**: Combination of L1 and L2 regularization

## Evaluation Metrics

Common metrics to evaluate linear regression models:
1. **Mean Absolute Error (MAE)**
2. **Mean Squared Error (MSE)**
3. **Root Mean Squared Error (RMSE)**
4. **R-squared (R²)**
5. **Adjusted R-squared**

## Python Implementation

### Simple Linear Regression Example

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Sample data
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Feature
y = np.array([2, 4, 5, 4, 5])                  # Target

# Create and fit the model
model = LinearRegression()
model.fit(X, y)

# Make predictions
y_pred = model.predict(X)

# Model evaluation
print(f"Coefficient: {model.coef_[0]}")
print(f"Intercept: {model.intercept_}")
print(f"MSE: {mean_squared_error(y, y_pred)}")
print(f"R²: {r2_score(y, y_pred)}")

# Plotting
plt.scatter(X, y, color='blue', label='Actual')
plt.plot(X, y_pred, color='red', label='Predicted')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()
```

### Multiple Linear Regression Example

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Sample dataset (Boston Housing is deprecated, using California housing instead)
from sklearn.datasets import fetch_california_housing
data = fetch_california_housing()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['Target'] = data.target

# Split data
X = df.drop('Target', axis=1)
y = df['Target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Evaluate
y_pred = model.predict(X_test_scaled)
print(f"R² Score: {r2_score(y_test, y_pred)}")
print(f"MSE: {mean_squared_error(y_test, y_pred)}")

# Display coefficients
coef_df = pd.DataFrame({
    'Feature': data.feature_names,
    'Coefficient': model.coef_
})
print(coef_df.sort_values(by='Coefficient', ascending=False))
```

### Regularized Linear Regression Example

```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet

# Ridge Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X_train_scaled, y_train)
print(f"Ridge R²: {ridge.score(X_test_scaled, y_test)}")

# Lasso Regression
lasso = Lasso(alpha=0.1)
lasso.fit(X_train_scaled, y_train)
print(f"Lasso R²: {lasso.score(X_test_scaled, y_test)}")

# Elastic Net
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic.fit(X_train_scaled, y_train)
print(f"Elastic Net R²: {elastic.score(X_test_scaled, y_test)}")
```

## Assumptions of Linear Regression

For reliable results, linear regression makes several assumptions:
1. **Linearity**: Relationship between features and target is linear
2. **Independence**: Observations are independent of each other
3. **Homoscedasticity**: Constant variance of errors
4. **Normality**: Errors are normally distributed
5. **No multicollinearity**: Features are not highly correlated with each other

## Advantages and Disadvantages

**Advantages:**
- Simple to implement and interpret
- Computationally efficient
- Works well when relationships are approximately linear
- Provides coefficients that indicate feature importance

**Disadvantages:**
- Sensitive to outliers
- Assumes linear relationship which may not hold in real-world scenarios
- Can underfit with complex relationships
- Multicollinearity can affect performance

## Practical Tips

1. Always check for linearity assumption with scatter plots
2. Handle outliers as they can significantly impact the model
3. Consider feature scaling for regularized regression
4. Use polynomial features if relationship is non-linear (Polynomial Regression)
5. Check for multicollinearity using VIF (Variance Inflation Factor)

Linear regression serves as an excellent starting point for regression problems and provides a foundation for understanding more complex machine learning algorithms.