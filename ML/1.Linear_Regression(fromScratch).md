# Linear Regression Implementation from Scratch using NumPy

Here's a complete implementation of linear regression using only NumPy:

```python
import numpy as np

class LinearRegression:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        """
        Initialize the linear regression model.
        
        Parameters:
        - learning_rate: float, the step size for gradient descent
        - n_iterations: int, number of iterations for gradient descent
        """
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        
    def fit(self, X, y):
        """
        Train the linear regression model using gradient descent.
        
        Parameters:
        - X: numpy array of shape (n_samples, n_features), input features
        - y: numpy array of shape (n_samples,), target values
        """
        n_samples, n_features = X.shape
        
        # Initialize parameters
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # Gradient descent
        for _ in range(self.n_iterations):
            # Predictions
            y_predicted = np.dot(X, self.weights) + self.bias
            
            # Compute gradients
            dw = (1/n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1/n_samples) * np.sum(y_predicted - y)
            
            # Update parameters
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict(self, X):
        """
        Predict target values for new data.
        
        Parameters:
        - X: numpy array of shape (n_samples, n_features), input features
        
        Returns:
        - numpy array of predicted values
        """
        return np.dot(X, self.weights) + self.bias
    
    def get_params(self):
        """
        Get the model parameters.
        
        Returns:
        - tuple of (weights, bias)
        """
        return self.weights, self.bias
```

## How to Use This Implementation

```python
# Example usage
if __name__ == "__main__":
    # Generate some sample data
    np.random.seed(0)
    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X + np.random.randn(100, 1)
    
    # Create and train the model
    model = LinearRegression(learning_rate=0.1, n_iterations=1000)
    model.fit(X, y)
    
    # Get the learned parameters
    weights, bias = model.get_params()
    print(f"Learned weights: {weights[0]:.4f}, Learned bias: {bias:.4f}")
    
    # Make predictions
    X_new = np.array([[0], [2]])
    y_pred = model.predict(X_new)
    print(f"Predictions for X = {X_new.ravel()}: {y_pred.ravel()}")
```

## Key Components Explained:

1. **Initialization**: The model starts with zero weights and bias.

2. **Gradient Descent**:
   - Computes predictions using current parameters
   - Calculates gradients for weights and bias
   - Updates parameters in the opposite direction of the gradients

3. **Prediction**: Uses the learned weights and bias to make new predictions

4. **Parameters**:
   - `learning_rate`: Controls how big each gradient descent step is
   - `n_iterations`: How many times to update the parameters

This implementation demonstrates the core concepts of linear regression:
- The hypothesis function (linear combination of weights and features)
- The cost function (mean squared error, implied in the gradient calculation)
- Gradient descent optimization

You can extend this basic implementation with features like:
- Regularization (L1/L2)
- Different optimization methods (SGD, mini-batch GD)
- Early stopping
- Feature scaling/normalization