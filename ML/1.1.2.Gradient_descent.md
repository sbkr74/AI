## **Gradient Descent for Linear Regression**

### **1. Problem Setup**
We want to fit a **linear regression** model:  
$$
y = w_1 x + w_0
$$  
where:
- \( $w_0$ \) = bias (intercept)
- \( w_1 \) = weight (slope)
- Goal: Find \( w_0 \) and \( w_1 \) that **minimize the Mean Squared Error (MSE)**.

### **2. Cost Function (MSE)**
$$
J(w_0, w_1) = \frac{1}{2m} \sum_{i=1}^{m} ( \hat{y}^{(i)} - y^{(i)} )^2
$$
where:
- \( \hat{y}^{(i)} = w_1 x^{(i)} + w_0 \) (prediction)
- \( m \) = number of training examples

### **3. Gradient Descent Steps**
We **update** the weights iteratively using:
$$
w_0 := w_0 - \alpha \frac{\partial J}{\partial w_0}
$$
$$
w_1 := w_1 - \alpha \frac{\partial J}{\partial w_1}
$$
where:
- \( \alpha \) = **learning rate** (controls step size)
- The **partial derivatives (gradients)** are:
  $$
  \frac{\partial J}{\partial w_0} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})
  $$
  $$
  \frac{\partial J}{\partial w_1} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)}) x^{(i)}
  $$

---

## **Python Implementation**
### **Step 1: Generate Synthetic Data**
```python
import numpy as np
import matplotlib.pyplot as plt

# Generate random data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)  # Features (0 to 2)
y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + noise

plt.scatter(X, y)
plt.xlabel("X")
plt.ylabel("y")
plt.title("Training Data")
plt.show()
```
**Output:**  
Randomly generated linear data with noise

---

### **Step 2: Implement Gradient Descent**
```python
def gradient_descent(X, y, learning_rate=0.1, n_iters=100):
    m = len(X)
    w0, w1 = 0, 0  # Initialize weights
    cost_history = []

    for _ in range(n_iters):
        y_pred = w1 * X + w0  # Predicted y
        error = y_pred - y     # Error
        
        # Compute gradients
        dw0 = (1/m) * np.sum(error)
        dw1 = (1/m) * np.sum(error * X)
        
        # Update weights
        w0 -= learning_rate * dw0
        w1 -= learning_rate * dw1
        
        # Compute and store cost (MSE)
        cost = (1/(2*m)) * np.sum(error**2)
        cost_history.append(cost)
    
    return w0, w1, cost_history

w0, w1, costs = gradient_descent(X, y, learning_rate=0.1, n_iters=100)
print(f"Optimal weights: w0 = {w0:.2f}, w1 = {w1:.2f}")
```
**Output:**  
```
Optimal weights: w0 = 4.21, w1 = 2.77
```
*(True values were \( w_0 = 4 \), \( w_1 = 3 \), but noise affects results.)*

---

### **Step 3: Plot the Regression Line**
```python
# Plot the best-fit line
plt.scatter(X, y)
plt.plot(X, w1 * X + w0, 'r-', label=f"y = {w1:.2f}x + {w0:.2f}")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()
plt.title("Linear Regression Fit")
plt.show()
```
**Output:**  
Best-fit regression line

---

### **Step 4: Check Cost Convergence**
```python
plt.plot(range(100), costs)
plt.xlabel("Iterations")
plt.ylabel("Cost (MSE)")
plt.title("Cost Function Convergence")
plt.show()
```
**Output:**  
Cost decreases over iterations

---

## **Key Takeaways**
1. **Gradient descent iteratively adjusts weights** to minimize the cost function (MSE).
2. **Learning rate (`α`)** must be chosen carefully:
   - Too small → Slow convergence
   - Too large → May overshoot minimum
3. **Batch Gradient Descent** computes gradients on the **entire dataset** (stable but slow for big data).
4. **Stochastic (SGD) or Mini-batch GD** are faster variants for large datasets.

---

## **When to Use Gradient Descent?**
| Scenario | Why Gradient Descent? |
|----------|----------------------|
| **Large datasets** | More efficient than normal equations (which require matrix inversion). |
| **Online learning** | Can update weights incrementally as new data arrives. |
| **Non-linear models (Neural Networks)** | Essential for optimizing complex models. |
| **Regularized regression (Ridge/Lasso)** | Works well with penalty terms. |

---

## **Comparison: Gradient Descent vs. Normal Equation**
| Method | Pros | Cons |
|--------|------|------|
| **Gradient Descent** | Works for big data, flexible | Needs hyperparameter tuning (`α`, iterations) |
| **Normal Equation** | Exact solution, no `α` needed | Slow for large features (O(n³) complexity) |

---

### **Final Thoughts**
Gradient descent is **fundamental** in machine learning, especially for regression and deep learning. Understanding it deeply helps in tuning models effectively.

